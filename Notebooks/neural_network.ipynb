{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_clean.pkl')\n",
    "test = pd.read_pickle('test_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_210</th>\n",
       "      <th>feature_211</th>\n",
       "      <th>feature_212</th>\n",
       "      <th>feature_213</th>\n",
       "      <th>feature_214</th>\n",
       "      <th>feature_215</th>\n",
       "      <th>feature_216</th>\n",
       "      <th>feature_217</th>\n",
       "      <th>feature_218</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.554222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190514</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.521013</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.299222</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.827157</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.545612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.248790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.851531</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206293</td>\n",
       "      <td>0.022686</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.116689</td>\n",
       "      <td>0.566809</td>\n",
       "      <td>0.020597</td>\n",
       "      <td>0.020891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.593774</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412052</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.087886</td>\n",
       "      <td>0.559571</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026009</td>\n",
       "      <td>0.074658</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>0.097360</td>\n",
       "      <td>0.020597</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160199</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>0.134400</td>\n",
       "      <td>0.841197</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049540</td>\n",
       "      <td>0.064444</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.85715</td>\n",
       "      <td>0.032973</td>\n",
       "      <td>0.131737</td>\n",
       "      <td>0.020597</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.554222        1.0        0.0   0.000000   0.398189   0.000000   \n",
       "1   0.521013        0.5        0.0   0.000691   0.299222   0.000649   \n",
       "2   0.545612        1.0        0.0   0.000102   0.248790   0.000000   \n",
       "3   0.593774        0.5        0.0   0.000000   0.412052   0.001299   \n",
       "4   0.278900        0.0        0.0   0.000000   0.160199   0.009740   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_210  feature_211  \\\n",
       "0   0.190514   0.774141   0.004995    0.666667  ...     0.000000     0.000000   \n",
       "1   0.000000   0.827157   0.000668    0.000000  ...     0.000000     0.000000   \n",
       "2   0.571429   0.851531   0.000604    0.000000  ...     0.206293     0.022686   \n",
       "3   0.087886   0.559571   0.002227    0.333333  ...     0.026009     0.074658   \n",
       "4   0.134400   0.841197   0.000700    0.833333  ...     0.049540     0.064444   \n",
       "\n",
       "   feature_212  feature_213  feature_214  feature_215  feature_216  \\\n",
       "0     0.000000      0.00000     0.000000     0.000000     0.000000   \n",
       "1     0.000000      0.00000     0.000000     0.000000     0.000000   \n",
       "2     0.000562      0.50000     0.116689     0.566809     0.020597   \n",
       "3     0.003375      1.00000     0.025595     0.097360     0.020597   \n",
       "4     0.003375      0.85715     0.032973     0.131737     0.020597   \n",
       "\n",
       "   feature_217  feature_218  target  \n",
       "0     0.000000          1.0       1  \n",
       "1     0.000000          1.0       1  \n",
       "2     0.020891          1.0       1  \n",
       "3     0.002580          0.0       1  \n",
       "4     0.002613          1.0       1  \n",
       "\n",
       "[5 rows x 178 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_205</th>\n",
       "      <th>feature_207</th>\n",
       "      <th>feature_208</th>\n",
       "      <th>feature_210</th>\n",
       "      <th>feature_211</th>\n",
       "      <th>feature_212</th>\n",
       "      <th>feature_214</th>\n",
       "      <th>feature_217</th>\n",
       "      <th>feature_218</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.485838</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.364916</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559775</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.471255</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432846</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.089721</td>\n",
       "      <td>0.740107</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792558</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.026187</td>\n",
       "      <td>0.026009</td>\n",
       "      <td>0.074658</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.619600</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208381</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.424221</td>\n",
       "      <td>0.668134</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.600626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.146909</td>\n",
       "      <td>0.015007</td>\n",
       "      <td>0.583363</td>\n",
       "      <td>0.858019</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789253</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.029193</td>\n",
       "      <td>0.176415</td>\n",
       "      <td>0.238134</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>0.043148</td>\n",
       "      <td>0.004473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.642833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.177191</td>\n",
       "      <td>0.042974</td>\n",
       "      <td>0.583363</td>\n",
       "      <td>0.691851</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783863</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.216927</td>\n",
       "      <td>0.211176</td>\n",
       "      <td>0.106760</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.122571</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.485838        0.5        0.0   0.000623   0.364916   0.000682   \n",
       "1   0.471255        0.5        0.0   0.000000   0.432846   0.001364   \n",
       "2   0.619600        0.5        0.0   0.000000   0.208381   0.002046   \n",
       "3   0.600626        1.0        0.0   0.000018   0.146909   0.015007   \n",
       "4   0.642833        1.0        0.0   0.000259   0.177191   0.042974   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_205  feature_207  \\\n",
       "0   0.000000   0.559775   0.000692    0.000000  ...     0.781460     0.000000   \n",
       "1   0.089721   0.740107   0.002308    0.333333  ...     0.792558     0.013420   \n",
       "2   0.424221   0.668134   0.000857    0.000000  ...     0.781460     0.000000   \n",
       "3   0.583363   0.858019   0.001088    0.500000  ...     0.789253     0.011287   \n",
       "4   0.583363   0.691851   0.000956    0.750000  ...     0.783863     0.000320   \n",
       "\n",
       "   feature_208  feature_210  feature_211  feature_212  feature_214  \\\n",
       "0     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "1     0.026187     0.026009     0.074658     0.003963     0.025595   \n",
       "2     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "3     0.029193     0.176415     0.238134     0.005945     0.043148   \n",
       "4     0.216927     0.211176     0.106760     0.001321     0.122571   \n",
       "\n",
       "   feature_217  feature_218  target  \n",
       "0     0.000000          1.0       1  \n",
       "1     0.002580          0.0       1  \n",
       "2     0.000000          1.0       1  \n",
       "3     0.004473          0.0       1  \n",
       "4     0.005346          0.0       1  \n",
       "\n",
       "[5 rows x 158 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = set(train.columns).intersection(set(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franz\\AppData\\Local\\Temp\\ipykernel_21272\\982039621.py:1: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  train = train[common_cols]\n",
      "C:\\Users\\franz\\AppData\\Local\\Temp\\ipykernel_21272\\982039621.py:3: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  test = test[common_cols]\n"
     ]
    }
   ],
   "source": [
    "train = train[common_cols]\n",
    "\n",
    "test = test[common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['target'])\n",
    "y_train = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(columns=['target'])\n",
    "y_test = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Sequential()\n",
    "\n",
    "modelo.add(Dense(128, activation = 'relu', kernel_regularizer = l2(0.01), input_shape = (X_train.shape[1],)))\n",
    "#modelo.add(Dense(128, activation = 'tanh'))\n",
    "#modelo.add(Dropout(0.2))\n",
    "modelo.add(Dense(64, activation = 'relu'))\n",
    "modelo.add(Dense(32, activation = 'relu'))\n",
    "modelo.add(Dense(16, activation = 'relu'))\n",
    "modelo.add(Dropout(0.2))\n",
    "\n",
    "modelo.add(Dense(1, activation = 'tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optim = Adam(learning_rate = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer = optim, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2128/2128 [==============================] - 18s 8ms/step - loss: 0.8352 - accuracy: 0.5122 - val_loss: 0.7015 - val_accuracy: 0.5328\n",
      "Epoch 2/500\n",
      "2128/2128 [==============================] - 18s 8ms/step - loss: 0.7035 - accuracy: 0.5145 - val_loss: 0.7469 - val_accuracy: 0.1515\n",
      "Epoch 3/500\n",
      "2128/2128 [==============================] - 18s 8ms/step - loss: 0.6946 - accuracy: 0.5146 - val_loss: 0.7134 - val_accuracy: 0.1590\n",
      "Epoch 4/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6932 - accuracy: 0.5148 - val_loss: 0.6787 - val_accuracy: 0.6611\n",
      "Epoch 5/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6927 - accuracy: 0.5194 - val_loss: 0.6814 - val_accuracy: 0.6129\n",
      "Epoch 6/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6913 - accuracy: 0.5355 - val_loss: 0.6952 - val_accuracy: 0.5389\n",
      "Epoch 7/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6887 - accuracy: 0.5472 - val_loss: 0.6176 - val_accuracy: 0.7668\n",
      "Epoch 8/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6873 - accuracy: 0.5530 - val_loss: 0.7059 - val_accuracy: 0.4234\n",
      "Epoch 9/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6871 - accuracy: 0.5550 - val_loss: 0.7260 - val_accuracy: 0.3935\n",
      "Epoch 10/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6867 - accuracy: 0.5576 - val_loss: 0.6685 - val_accuracy: 0.5632\n",
      "Epoch 11/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6856 - accuracy: 0.5602 - val_loss: 0.7507 - val_accuracy: 0.2954\n",
      "Epoch 12/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6855 - accuracy: 0.5623 - val_loss: 0.6165 - val_accuracy: 0.7222\n",
      "Epoch 13/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6847 - accuracy: 0.5653 - val_loss: 0.5851 - val_accuracy: 0.7692\n",
      "Epoch 14/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6839 - accuracy: 0.5694 - val_loss: 0.7458 - val_accuracy: 0.3248\n",
      "Epoch 15/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6821 - accuracy: 0.5733 - val_loss: 0.6417 - val_accuracy: 0.6600\n",
      "Epoch 16/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6819 - accuracy: 0.5744 - val_loss: 0.6130 - val_accuracy: 0.7228\n",
      "Epoch 17/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6820 - accuracy: 0.5744 - val_loss: 0.6292 - val_accuracy: 0.6816\n",
      "Epoch 18/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6825 - accuracy: 0.5733 - val_loss: 0.6851 - val_accuracy: 0.5437\n",
      "Epoch 19/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6806 - accuracy: 0.5787 - val_loss: 0.6575 - val_accuracy: 0.6208\n",
      "Epoch 20/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6826 - accuracy: 0.5739 - val_loss: 0.7060 - val_accuracy: 0.4774\n",
      "Epoch 21/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6821 - accuracy: 0.5751 - val_loss: 0.6837 - val_accuracy: 0.5471\n",
      "Epoch 22/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6808 - accuracy: 0.5786 - val_loss: 0.7037 - val_accuracy: 0.5100\n",
      "Epoch 23/500\n",
      "2128/2128 [==============================] - 18s 8ms/step - loss: 0.6811 - accuracy: 0.5764 - val_loss: 0.6228 - val_accuracy: 0.7149\n",
      "Epoch 24/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6812 - accuracy: 0.5787 - val_loss: 0.6786 - val_accuracy: 0.5632\n",
      "Epoch 25/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6807 - accuracy: 0.5773 - val_loss: 0.5998 - val_accuracy: 0.7579\n",
      "Epoch 26/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6806 - accuracy: 0.5786 - val_loss: 0.6241 - val_accuracy: 0.6748\n",
      "Epoch 27/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6795 - accuracy: 0.5813 - val_loss: 0.7321 - val_accuracy: 0.4182\n",
      "Epoch 28/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6808 - accuracy: 0.5776 - val_loss: 0.6538 - val_accuracy: 0.6222\n",
      "Epoch 29/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6801 - accuracy: 0.5798 - val_loss: 0.5628 - val_accuracy: 0.8064\n",
      "Epoch 30/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6805 - accuracy: 0.5801 - val_loss: 0.5645 - val_accuracy: 0.7980\n",
      "Epoch 31/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6815 - accuracy: 0.5777 - val_loss: 0.6616 - val_accuracy: 0.5870\n",
      "Epoch 32/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6798 - accuracy: 0.5813 - val_loss: 0.5681 - val_accuracy: 0.8117\n",
      "Epoch 33/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6809 - accuracy: 0.5783 - val_loss: 0.5919 - val_accuracy: 0.7520\n",
      "Epoch 34/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6807 - accuracy: 0.5803 - val_loss: 0.5109 - val_accuracy: 0.8313\n",
      "Epoch 35/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6808 - accuracy: 0.5782 - val_loss: 0.7295 - val_accuracy: 0.4347\n",
      "Epoch 36/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6818 - accuracy: 0.5767 - val_loss: 0.6170 - val_accuracy: 0.7050\n",
      "Epoch 37/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6820 - accuracy: 0.5792 - val_loss: 0.6495 - val_accuracy: 0.6317\n",
      "Epoch 38/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6809 - accuracy: 0.5797 - val_loss: 0.7188 - val_accuracy: 0.4440\n",
      "Epoch 39/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6797 - accuracy: 0.5811 - val_loss: 0.6461 - val_accuracy: 0.6442\n",
      "Epoch 40/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6807 - accuracy: 0.5780 - val_loss: 0.6844 - val_accuracy: 0.5362\n",
      "Epoch 41/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6808 - accuracy: 0.5778 - val_loss: 0.7755 - val_accuracy: 0.3506\n",
      "Epoch 42/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6797 - accuracy: 0.5808 - val_loss: 0.5944 - val_accuracy: 0.7593\n",
      "Epoch 43/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6804 - accuracy: 0.5787 - val_loss: 0.7020 - val_accuracy: 0.5127\n",
      "Epoch 44/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6804 - accuracy: 0.5800 - val_loss: 0.6289 - val_accuracy: 0.6891\n",
      "Epoch 45/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6802 - accuracy: 0.5830 - val_loss: 0.6934 - val_accuracy: 0.5363\n",
      "Epoch 46/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6793 - accuracy: 0.5820 - val_loss: 0.6512 - val_accuracy: 0.6272\n",
      "Epoch 47/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6798 - accuracy: 0.5811 - val_loss: 0.6343 - val_accuracy: 0.7050\n",
      "Epoch 48/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6805 - accuracy: 0.5799 - val_loss: 0.6637 - val_accuracy: 0.6185\n",
      "Epoch 49/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6807 - accuracy: 0.5795 - val_loss: 0.5695 - val_accuracy: 0.8093\n",
      "Epoch 50/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6795 - accuracy: 0.5812 - val_loss: 0.6542 - val_accuracy: 0.6300\n",
      "Epoch 51/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6791 - accuracy: 0.5827 - val_loss: 0.6866 - val_accuracy: 0.5460\n",
      "Epoch 52/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6805 - accuracy: 0.5776 - val_loss: 0.6819 - val_accuracy: 0.5562\n",
      "Epoch 53/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6802 - accuracy: 0.5795 - val_loss: 0.6105 - val_accuracy: 0.7272\n",
      "Epoch 54/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6804 - accuracy: 0.5792 - val_loss: 0.6177 - val_accuracy: 0.6958\n",
      "Epoch 55/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6802 - accuracy: 0.5791 - val_loss: 0.6371 - val_accuracy: 0.6482\n",
      "Epoch 56/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6801 - accuracy: 0.5809 - val_loss: 0.6970 - val_accuracy: 0.5060\n",
      "Epoch 57/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6808 - accuracy: 0.5796 - val_loss: 0.6558 - val_accuracy: 0.6249\n",
      "Epoch 58/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6808 - accuracy: 0.5795 - val_loss: 0.6923 - val_accuracy: 0.5268\n",
      "Epoch 59/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6802 - accuracy: 0.5808 - val_loss: 0.7096 - val_accuracy: 0.4667\n",
      "Epoch 60/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6804 - accuracy: 0.5799 - val_loss: 0.6388 - val_accuracy: 0.6494\n",
      "Epoch 61/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6800 - accuracy: 0.5797 - val_loss: 0.5947 - val_accuracy: 0.7524\n",
      "Epoch 62/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6811 - accuracy: 0.5799 - val_loss: 0.7192 - val_accuracy: 0.4701\n",
      "Epoch 63/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6808 - accuracy: 0.5799 - val_loss: 0.6613 - val_accuracy: 0.6170\n",
      "Epoch 64/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6789 - accuracy: 0.5813 - val_loss: 0.6479 - val_accuracy: 0.6416\n",
      "Epoch 65/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6828 - accuracy: 0.5780 - val_loss: 0.7593 - val_accuracy: 0.1444\n",
      "Epoch 66/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6882 - accuracy: 0.5568 - val_loss: 0.6740 - val_accuracy: 0.5857\n",
      "Epoch 67/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6805 - accuracy: 0.5787 - val_loss: 0.6136 - val_accuracy: 0.7337\n",
      "Epoch 68/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6807 - accuracy: 0.5786 - val_loss: 0.6090 - val_accuracy: 0.7198\n",
      "Epoch 69/500\n",
      "2128/2128 [==============================] - 18s 9ms/step - loss: 0.6809 - accuracy: 0.5786 - val_loss: 0.6516 - val_accuracy: 0.6424\n",
      "Epoch 70/500\n",
      "2128/2128 [==============================] - 19s 9ms/step - loss: 0.6800 - accuracy: 0.5812 - val_loss: 0.6787 - val_accuracy: 0.5724\n",
      "Epoch 71/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6797 - accuracy: 0.5812 - val_loss: 0.6518 - val_accuracy: 0.6205\n",
      "Epoch 72/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6808 - accuracy: 0.5787 - val_loss: 0.7516 - val_accuracy: 0.4153\n",
      "Epoch 73/500\n",
      "2128/2128 [==============================] - 22s 11ms/step - loss: 0.6803 - accuracy: 0.5784 - val_loss: 0.8138 - val_accuracy: 0.2621\n",
      "Epoch 74/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6804 - accuracy: 0.5798 - val_loss: 0.7154 - val_accuracy: 0.4958\n",
      "Epoch 75/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6812 - accuracy: 0.5769 - val_loss: 0.7093 - val_accuracy: 0.4900\n",
      "Epoch 76/500\n",
      "2128/2128 [==============================] - 22s 11ms/step - loss: 0.6813 - accuracy: 0.5762 - val_loss: 0.6162 - val_accuracy: 0.7550\n",
      "Epoch 77/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6807 - accuracy: 0.5776 - val_loss: 0.6276 - val_accuracy: 0.6867\n",
      "Epoch 78/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6811 - accuracy: 0.5779 - val_loss: 0.7120 - val_accuracy: 0.4623\n",
      "Epoch 79/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6804 - accuracy: 0.5791 - val_loss: 0.6388 - val_accuracy: 0.6729\n",
      "Epoch 80/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6803 - accuracy: 0.5804 - val_loss: 0.6059 - val_accuracy: 0.7203\n",
      "Epoch 81/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6803 - accuracy: 0.5802 - val_loss: 0.8374 - val_accuracy: 0.2151\n",
      "Epoch 82/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6810 - accuracy: 0.5783 - val_loss: 0.5992 - val_accuracy: 0.7548\n",
      "Epoch 83/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6816 - accuracy: 0.5774 - val_loss: 0.6905 - val_accuracy: 0.5453\n",
      "Epoch 84/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6809 - accuracy: 0.5779 - val_loss: 0.6002 - val_accuracy: 0.7509\n",
      "Epoch 85/500\n",
      "2128/2128 [==============================] - 20s 9ms/step - loss: 0.6799 - accuracy: 0.5814 - val_loss: 0.7128 - val_accuracy: 0.4671\n",
      "Epoch 86/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6810 - accuracy: 0.5794 - val_loss: 0.7867 - val_accuracy: 0.3461\n",
      "Epoch 87/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6801 - accuracy: 0.5809 - val_loss: 0.6113 - val_accuracy: 0.7245\n",
      "Epoch 88/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6810 - accuracy: 0.5787 - val_loss: 0.5803 - val_accuracy: 0.7793\n",
      "Epoch 89/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6796 - accuracy: 0.5819 - val_loss: 0.7075 - val_accuracy: 0.4870\n",
      "Epoch 90/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6802 - accuracy: 0.5815 - val_loss: 0.6284 - val_accuracy: 0.6706\n",
      "Epoch 91/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6802 - accuracy: 0.5806 - val_loss: 0.7872 - val_accuracy: 0.3430\n",
      "Epoch 92/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6791 - accuracy: 0.5814 - val_loss: 0.7229 - val_accuracy: 0.4725\n",
      "Epoch 93/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6804 - accuracy: 0.5785 - val_loss: 0.5803 - val_accuracy: 0.7582\n",
      "Epoch 94/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6801 - accuracy: 0.5797 - val_loss: 0.7624 - val_accuracy: 0.4019\n",
      "Epoch 95/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6805 - accuracy: 0.5800 - val_loss: 0.6742 - val_accuracy: 0.5626\n",
      "Epoch 96/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6805 - accuracy: 0.5782 - val_loss: 0.6653 - val_accuracy: 0.6033\n",
      "Epoch 97/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6799 - accuracy: 0.5808 - val_loss: 0.6136 - val_accuracy: 0.7029\n",
      "Epoch 98/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6811 - accuracy: 0.5796 - val_loss: 0.6027 - val_accuracy: 0.7432\n",
      "Epoch 99/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6791 - accuracy: 0.5824 - val_loss: 0.8156 - val_accuracy: 0.2689\n",
      "Epoch 100/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6802 - accuracy: 0.5813 - val_loss: 0.5565 - val_accuracy: 0.7977\n",
      "Epoch 101/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6802 - accuracy: 0.5802 - val_loss: 0.5636 - val_accuracy: 0.7926\n",
      "Epoch 102/500\n",
      "2128/2128 [==============================] - 22s 11ms/step - loss: 0.6812 - accuracy: 0.5800 - val_loss: 0.7400 - val_accuracy: 0.3756\n",
      "Epoch 103/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6807 - accuracy: 0.5800 - val_loss: 0.7462 - val_accuracy: 0.4128\n",
      "Epoch 104/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6816 - accuracy: 0.5791 - val_loss: 0.6641 - val_accuracy: 0.5910\n",
      "Epoch 105/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6812 - accuracy: 0.5785 - val_loss: 0.6721 - val_accuracy: 0.5775\n",
      "Epoch 106/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6801 - accuracy: 0.5800 - val_loss: 0.5963 - val_accuracy: 0.7537\n",
      "Epoch 107/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6800 - accuracy: 0.5803 - val_loss: 0.6838 - val_accuracy: 0.5477\n",
      "Epoch 108/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6804 - accuracy: 0.5781 - val_loss: 0.5877 - val_accuracy: 0.7577\n",
      "Epoch 109/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6812 - accuracy: 0.5770 - val_loss: 0.6852 - val_accuracy: 0.5520\n",
      "Epoch 110/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6809 - accuracy: 0.5783 - val_loss: 0.6343 - val_accuracy: 0.6620\n",
      "Epoch 111/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6805 - accuracy: 0.5784 - val_loss: 0.7174 - val_accuracy: 0.4456\n",
      "Epoch 112/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6799 - accuracy: 0.5809 - val_loss: 0.6809 - val_accuracy: 0.5609\n",
      "Epoch 113/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6800 - accuracy: 0.5804 - val_loss: 0.6300 - val_accuracy: 0.6966\n",
      "Epoch 114/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6799 - accuracy: 0.5802 - val_loss: 0.7539 - val_accuracy: 0.3806\n",
      "Epoch 115/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6808 - accuracy: 0.5802 - val_loss: 0.6928 - val_accuracy: 0.5311\n",
      "Epoch 116/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6807 - accuracy: 0.5780 - val_loss: 0.6744 - val_accuracy: 0.5767\n",
      "Epoch 117/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6810 - accuracy: 0.5791 - val_loss: 0.6329 - val_accuracy: 0.6793\n",
      "Epoch 118/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6803 - accuracy: 0.5802 - val_loss: 0.6287 - val_accuracy: 0.7232\n",
      "Epoch 119/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6795 - accuracy: 0.5811 - val_loss: 0.7147 - val_accuracy: 0.4920\n",
      "Epoch 120/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6805 - accuracy: 0.5789 - val_loss: 0.7261 - val_accuracy: 0.4219\n",
      "Epoch 121/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6799 - accuracy: 0.5798 - val_loss: 0.6728 - val_accuracy: 0.5886\n",
      "Epoch 122/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6795 - accuracy: 0.5813 - val_loss: 0.6613 - val_accuracy: 0.6164\n",
      "Epoch 123/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6792 - accuracy: 0.5831 - val_loss: 0.7571 - val_accuracy: 0.4107\n",
      "Epoch 124/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6799 - accuracy: 0.5793 - val_loss: 0.6669 - val_accuracy: 0.5944\n",
      "Epoch 125/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6808 - accuracy: 0.5809 - val_loss: 0.7293 - val_accuracy: 0.4453\n",
      "Epoch 126/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6812 - accuracy: 0.5756 - val_loss: 0.6604 - val_accuracy: 0.6180\n",
      "Epoch 127/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6806 - accuracy: 0.5806 - val_loss: 0.7595 - val_accuracy: 0.3708\n",
      "Epoch 128/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6812 - accuracy: 0.5775 - val_loss: 0.6877 - val_accuracy: 0.5541\n",
      "Epoch 129/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6800 - accuracy: 0.5811 - val_loss: 0.6993 - val_accuracy: 0.5156\n",
      "Epoch 130/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6813 - accuracy: 0.5791 - val_loss: 0.7719 - val_accuracy: 0.3477\n",
      "Epoch 131/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6814 - accuracy: 0.5770 - val_loss: 0.7151 - val_accuracy: 0.4653\n",
      "Epoch 132/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6817 - accuracy: 0.5768 - val_loss: 0.6614 - val_accuracy: 0.6127\n",
      "Epoch 133/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6806 - accuracy: 0.5804 - val_loss: 0.5997 - val_accuracy: 0.7444\n",
      "Epoch 134/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6805 - accuracy: 0.5786 - val_loss: 0.6900 - val_accuracy: 0.5307\n",
      "Epoch 135/500\n",
      "2128/2128 [==============================] - 22s 11ms/step - loss: 0.6806 - accuracy: 0.5797 - val_loss: 0.7381 - val_accuracy: 0.3891\n",
      "Epoch 136/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6804 - accuracy: 0.5805 - val_loss: 0.6926 - val_accuracy: 0.5233\n",
      "Epoch 137/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6798 - accuracy: 0.5793 - val_loss: 0.6996 - val_accuracy: 0.5041\n",
      "Epoch 138/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6798 - accuracy: 0.5814 - val_loss: 0.7123 - val_accuracy: 0.4930\n",
      "Epoch 139/500\n",
      "2128/2128 [==============================] - 22s 10ms/step - loss: 0.6810 - accuracy: 0.5796 - val_loss: 0.6699 - val_accuracy: 0.5965\n",
      "Epoch 140/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6817 - accuracy: 0.5763 - val_loss: 0.6022 - val_accuracy: 0.7369\n",
      "Epoch 141/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6810 - accuracy: 0.5786 - val_loss: 0.6909 - val_accuracy: 0.5359\n",
      "Epoch 142/500\n",
      "2128/2128 [==============================] - 25s 12ms/step - loss: 0.6814 - accuracy: 0.5781 - val_loss: 0.7091 - val_accuracy: 0.4924\n",
      "Epoch 143/500\n",
      "2128/2128 [==============================] - 25s 12ms/step - loss: 0.6805 - accuracy: 0.5794 - val_loss: 0.7811 - val_accuracy: 0.3650\n",
      "Epoch 144/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6797 - accuracy: 0.5815 - val_loss: 0.5880 - val_accuracy: 0.7470\n",
      "Epoch 145/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6794 - accuracy: 0.5820 - val_loss: 0.5665 - val_accuracy: 0.8036\n",
      "Epoch 146/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6794 - accuracy: 0.5820 - val_loss: 0.6545 - val_accuracy: 0.6236\n",
      "Epoch 147/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6794 - accuracy: 0.5825 - val_loss: 0.6832 - val_accuracy: 0.5366\n",
      "Epoch 148/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6807 - accuracy: 0.5798 - val_loss: 0.7618 - val_accuracy: 0.3467\n",
      "Epoch 149/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6811 - accuracy: 0.5777 - val_loss: 0.7018 - val_accuracy: 0.4927\n",
      "Epoch 150/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6807 - accuracy: 0.5797 - val_loss: 0.6713 - val_accuracy: 0.5762\n",
      "Epoch 151/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6809 - accuracy: 0.5776 - val_loss: 0.6734 - val_accuracy: 0.5574\n",
      "Epoch 152/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6814 - accuracy: 0.5787 - val_loss: 0.6109 - val_accuracy: 0.7234\n",
      "Epoch 153/500\n",
      "2128/2128 [==============================] - 21s 10ms/step - loss: 0.6815 - accuracy: 0.5775 - val_loss: 0.6600 - val_accuracy: 0.6131\n",
      "Epoch 154/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6813 - accuracy: 0.5780 - val_loss: 0.6488 - val_accuracy: 0.6446\n",
      "Epoch 155/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6807 - accuracy: 0.5790 - val_loss: 0.6396 - val_accuracy: 0.6630\n",
      "Epoch 156/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6800 - accuracy: 0.5803 - val_loss: 0.6855 - val_accuracy: 0.5478\n",
      "Epoch 157/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6810 - accuracy: 0.5789 - val_loss: 0.5875 - val_accuracy: 0.7685\n",
      "Epoch 158/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6803 - accuracy: 0.5791 - val_loss: 0.6834 - val_accuracy: 0.5381\n",
      "Epoch 159/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6813 - accuracy: 0.5769 - val_loss: 0.7680 - val_accuracy: 0.3347\n",
      "Epoch 160/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6819 - accuracy: 0.5760 - val_loss: 0.6990 - val_accuracy: 0.5002\n",
      "Epoch 161/500\n",
      "2128/2128 [==============================] - 25s 12ms/step - loss: 0.6806 - accuracy: 0.5797 - val_loss: 0.6530 - val_accuracy: 0.6352\n",
      "Epoch 162/500\n",
      "2128/2128 [==============================] - 25s 12ms/step - loss: 0.6818 - accuracy: 0.5772 - val_loss: 0.7147 - val_accuracy: 0.4697\n",
      "Epoch 163/500\n",
      "2128/2128 [==============================] - 26s 12ms/step - loss: 0.6792 - accuracy: 0.5801 - val_loss: 0.5520 - val_accuracy: 0.8142\n",
      "Epoch 164/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6819 - accuracy: 0.5767 - val_loss: 0.7001 - val_accuracy: 0.5061\n",
      "Epoch 165/500\n",
      "2128/2128 [==============================] - 26s 12ms/step - loss: 0.6798 - accuracy: 0.5801 - val_loss: 0.6084 - val_accuracy: 0.7290\n",
      "Epoch 166/500\n",
      "2128/2128 [==============================] - 25s 12ms/step - loss: 0.6812 - accuracy: 0.5782 - val_loss: 0.7072 - val_accuracy: 0.4787\n",
      "Epoch 167/500\n",
      "2128/2128 [==============================] - 25s 12ms/step - loss: 0.6807 - accuracy: 0.5791 - val_loss: 0.6313 - val_accuracy: 0.6778\n",
      "Epoch 168/500\n",
      "2128/2128 [==============================] - 26s 12ms/step - loss: 0.6803 - accuracy: 0.5791 - val_loss: 0.5955 - val_accuracy: 0.7319\n",
      "Epoch 169/500\n",
      "2128/2128 [==============================] - 26s 12ms/step - loss: 0.6797 - accuracy: 0.5820 - val_loss: 0.6797 - val_accuracy: 0.5563\n",
      "Epoch 170/500\n",
      "2128/2128 [==============================] - 26s 12ms/step - loss: 0.6806 - accuracy: 0.5789 - val_loss: 0.6590 - val_accuracy: 0.6090\n",
      "Epoch 171/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6804 - accuracy: 0.5790 - val_loss: 0.5893 - val_accuracy: 0.7588\n",
      "Epoch 172/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6802 - accuracy: 0.5808 - val_loss: 0.5886 - val_accuracy: 0.7592\n",
      "Epoch 173/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6794 - accuracy: 0.5820 - val_loss: 0.8104 - val_accuracy: 0.3000\n",
      "Epoch 174/500\n",
      "2128/2128 [==============================] - 27s 13ms/step - loss: 0.6799 - accuracy: 0.5805 - val_loss: 0.8363 - val_accuracy: 0.2364\n",
      "Epoch 175/500\n",
      "2128/2128 [==============================] - 26s 12ms/step - loss: 0.6790 - accuracy: 0.5803 - val_loss: 0.6416 - val_accuracy: 0.6375\n",
      "Epoch 176/500\n",
      "2128/2128 [==============================] - 26s 12ms/step - loss: 0.6799 - accuracy: 0.5797 - val_loss: 0.6889 - val_accuracy: 0.5451\n",
      "Epoch 177/500\n",
      "2128/2128 [==============================] - 25s 12ms/step - loss: 0.6798 - accuracy: 0.5817 - val_loss: 0.6327 - val_accuracy: 0.6747\n",
      "Epoch 178/500\n",
      "2128/2128 [==============================] - 25s 12ms/step - loss: 0.6796 - accuracy: 0.5823 - val_loss: 0.6264 - val_accuracy: 0.6739\n",
      "Epoch 179/500\n",
      "2128/2128 [==============================] - 26s 12ms/step - loss: 0.6792 - accuracy: 0.5840 - val_loss: 0.7144 - val_accuracy: 0.4740\n",
      "Epoch 180/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6791 - accuracy: 0.5824 - val_loss: 0.6994 - val_accuracy: 0.5129\n",
      "Epoch 181/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6792 - accuracy: 0.5839 - val_loss: 0.6894 - val_accuracy: 0.5422\n",
      "Epoch 182/500\n",
      "2128/2128 [==============================] - 25s 12ms/step - loss: 0.6800 - accuracy: 0.5831 - val_loss: 0.6249 - val_accuracy: 0.6985\n",
      "Epoch 183/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6806 - accuracy: 0.5804 - val_loss: 0.7320 - val_accuracy: 0.4461\n",
      "Epoch 184/500\n",
      "2128/2128 [==============================] - 24s 11ms/step - loss: 0.6803 - accuracy: 0.5794 - val_loss: 0.5609 - val_accuracy: 0.7859\n",
      "Epoch 185/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6808 - accuracy: 0.5787 - val_loss: 0.6890 - val_accuracy: 0.5402\n",
      "Epoch 186/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6806 - accuracy: 0.5781 - val_loss: 0.5421 - val_accuracy: 0.8245\n",
      "Epoch 187/500\n",
      "2128/2128 [==============================] - 23s 11ms/step - loss: 0.6793 - accuracy: 0.5840 - val_loss: 0.7224 - val_accuracy: 0.4559\n",
      "Epoch 188/500\n",
      "2126/2128 [============================>.] - ETA: 0s - loss: 0.6797 - accuracy: 0.5808"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\franz\\Desktop\\Baubap\\baubap-nn-challenge\\Notebooks\\neural_network.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franz/Desktop/Baubap/baubap-nn-challenge/Notebooks/neural_network.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m modelo\u001b[39m.\u001b[39;49mfit(X_train_resampled, y_train_resampled, epochs \u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m, validation_data \u001b[39m=\u001b[39;49m (X_val, y_val), verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1607\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1608\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1609\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1610\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1611\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1612\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1613\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1614\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1615\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1616\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1617\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1618\u001b[0m )\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[0;32m   1948\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\franz\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = modelo.fit(X_train_resampled, y_train_resampled, epochs = 500, batch_size = 50, validation_data = (X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
